{
  "hash": "a86e78275430a9544bc7d3390f8d25e3",
  "result": {
    "markdown": "---\ntitle: \"Regression\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(ggfortify)\nlibrary(DHARMa)\n```\n:::\n\n\n\n**Lineare Regression**\n\n+ gerichtete Abhängigkeit zwischen zwei Variablen\n+ ${x}$ beeinflusst ${y}$ \n+ ${x}$ unabhängige Variable (independent, predictor, explanatory variable)\n+ ${y}$ abhängige Variable (dependent, response variable)\n+ sowohl Abhängige als auch Erklärungsvariable sind kontinuierlich\n+ ${y=f(x)}$  oder `y ~ x`\n+ ${y = a + bx}$\n+ Parameter ${a}$ Intercept (Achsenabschnitt)\n+ Parameter ${b}$ Slope (Steigung)\n+ `mod<-lm(Abhängige~Erklärungsvariable, data=md)`\n+ Differenz zwischen gemessenem ${y}$  (`observed` oder `measured`) und dem vom Modell vorhergesagten Wert $\\hat{y}$  (`fitted` oder `predicted`) beim gleichen Wert ${x}$ nennt man Residuen (`residuals`)\n+ Residuen haben die gleiche Einheit wie ${y}$ (parallel zur y-Achse)  \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-2-1.png){width=576}\n:::\n:::\n\n\n\nAnnahmen: \n\n+ die Erklärungsvariable x wurde fehlerfrei gemessen\n+ Varianz von y ist konstant (wird nicht größer)\n+ Residuen annähernd normalverteilt \n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.Bsp1=lm(yn~x)\nsimulationOutput <- simulateResiduals(fittedModel = mod.Bsp1, plot = F)\nplot(simulationOutput)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmod.Bsp2=lm(yh~x)\nsimulationOutput <- simulateResiduals(fittedModel = mod.Bsp2, plot = F)\nplot(simulationOutput)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\nNur für das erste Beispiel treffen die Annahmen für eine Regression zu. Im zweiten Beispiel tritt der Trompeteneffekt auf (Heteroskedastizität).   \n\n\n### Beispiel Anscombe 1973\n\n\n::: {.cell fig.asp='1'}\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-6-1.png){width=576}\n:::\n:::\n\n\nDiese vier Datensätze ergeben vier Regressionsmodelle mit gleichem Intercept, Slope, R² und Stichprobenumfang. Doch nur für das Beispiel oben links werden die Annahmen für eine lineare Regression erfüllt. \nDieses Beispiel verdeutlicht, dass wir dringend Modelldiagnostik betreiben müssen, indem wir unsere Daten und die Residuen der Modelle plotten.   \n\n\n## Beispiel Trade-off zwischen Ertrag und Proteingehalt\n\n\nBei gleicher N-Düngung beobachtet man im Weizen aufgrund unterschiedlicher Sorteneigenschaften häufig einen Trade-off zwischen Ertrag und Proteingehalt. \n\nFrage: Wie stark reduziert sich der Proteingehalt mit steigendem Ertrag?\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n# Vorgehensweise:  \n\n[Trade-off.xlsx](https://github.com/DoreenGabriel/Kurs/blob/main/Themen/05/Trade-off.xlsx){target=\"_blank\"} \n\n## Daten einlesen, kennenlernen und plotten\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(openxlsx)\nreg<-read.xlsx(\"Trade-off.xlsx\")\nstr(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t10 obs. of  2 variables:\n $ Ert : num  90.5 101.3 93.3 102 72 ...\n $ Prot: num  10.9 12.5 12.3 11.6 13.3 ...\n```\n:::\n\n```{.r .cell-code}\nsummary(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Ert              Prot      \n Min.   : 51.65   Min.   :10.51  \n 1st Qu.: 72.88   1st Qu.:11.73  \n Median : 90.70   Median :12.89  \n Mean   : 84.93   Mean   :13.12  \n 3rd Qu.: 99.30   3rd Qu.:14.07  \n Max.   :109.28   Max.   :16.80  \n```\n:::\n\n```{.r .cell-code}\nggplot(reg, aes(x=Ert, y=Prot)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n## Modell formulieren\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod<-lm(Prot~Ert, data=reg)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Prot ~ Ert, data = reg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.64733 -0.49234  0.00472  0.60021  1.54734 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21.26794    1.55972  13.636 8.05e-07 ***\nErt         -0.09590    0.01798  -5.335 0.000699 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.008 on 8 degrees of freedom\nMultiple R-squared:  0.7806,\tAdjusted R-squared:  0.7531 \nF-statistic: 28.46 on 1 and 8 DF,  p-value: 0.0006987\n```\n:::\n:::\n\n\n\nMit jedem Anstieg des Ertrag (je dt/ha) sinkt der Proteingehalt um -0.1 %. \nDas R² des Modells beträgt 78.1. \n\n\n## Signifikanztest der Modellparameter\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(mod, test=\"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nProt ~ Ert\n       Df Sum of Sq    RSS     AIC F value    Pr(>F)    \n<none>               8.128  1.9277                      \nErt     1    28.914 37.043 15.0948  28.458 0.0006987 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nSignifikanter Zusammenhang zwischen Ertrag und Proteingehalt.  \n\n## Modelldiagnostik\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DHARMa)\nsimulationOutput <- simulateResiduals(fittedModel = mod, plot = F)\nplot(simulationOutput)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nAuch wenn es bei diesem kleinen Stichprobenumfang schwierig ist, diese Plots sicher zu interpretieren, scheint alles in Ordnung zu sein.  \n\n+ Die Residuen sind annähernd normalverteilt (Plot oben links).\n+ Die Residuen weisen keinen Trompeteneffekt (Varianzheterogenität) auf (Plot oben rechts).\n+ Es gibt keine erkennbare Muster in den Residuen.\n+ Es gibt keine einflussreiche Punkte (keine roten Sternchen im Plot oben rechts).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotResiduals(simulationOutput, form = reg$Ert)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\nAuch der Plot gegen die Erklärungsvariable zeigt keine auffälligen Muster. \n\n\nDas Paket `ggfortify` gibt noch zwei weitere Plots zur Cook's Distance und Leverage aus. Hat eine Stichprobe eine hohe Leverage (i.e. Hebelwirkung, extremer Wert in x) und gleichzeitig ein großes Residuum (große Differenz zwischen beobachtetem und erwartetem Wert), dann spricht man von einem einflussreichem Punkt, der evtl. ein Ausreißer ist und durch eine hohe Cook's Distance (> 1 oder 0,5) gekennzeichnet ist.  \nEntsprechend kann man auf diese Werte nochmal genauer schauen (i.e. den Wert auf Eingabefehler überprüfen) und ggfls. das Modell ohne Ausreißer rechnen und die \"neuen\" Modellparameter mit den \"alten\" vergleichen und damit die Robustheit der Ergebnisse überprüfen.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggfortify)\nautoplot(mod, which =c(4,6), ncol = 2, label.size = 3)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n## Modellinterpretation\n\nDie `predict`-Funktion rechnet uns die Erwartungswerte basierend auf den Modellkoeffizienten aus. Gibt man kein weiteres Argument in die `predict`-Funktion, dann werden die Originaldaten zur Vorhersage genutzt.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(mod) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3        4        5        6        7        8 \n12.59231 11.55265 12.32326 11.48316 14.36743 16.31497 15.24944 14.01335 \n       9       10 \n12.54665 10.78760 \n```\n:::\n\n```{.r .cell-code}\nreg$Ert\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  90.46327 101.30412  93.26876 102.02872  71.95367  51.64602  62.75668\n [8]  75.64570  90.93937 109.28159\n```\n:::\n:::\n\n\nBei einem Ertrag von 90.4 dt/ha schätzt unser Modell einen Proteingehalt von 12.59 %, bei einem Ertrag von 101.3 dt/ha schätzt es einen Proteingehalt von 11.6 %. \n\nWir können uns nun fragen, wie hoch der Proteingehalt bei einem Ertrag von 80 dt/ha ist. Hierzu müssen wir die geschätzten Koeffizienten (`r `coef(mod)`) in die Modellgleichung (y = a + b*x) einsetzen, wobei a der Intercept, b der Koeffizient für `Ert` und x der Ertrag ist:  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(mod, newdata=data.frame(Ert=80))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n13.59576 \n```\n:::\n:::\n\n\nJetzt fehlt nur noch eine Abbildung zum Zusammenhang zwischen Wachstum und Ertrag.  \nGanz schnell und einfach geht es mit dem Package `effects`.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(effects)\nplot(allEffects(mod))\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(c(\"Ert\"), mod, partial.residuals=TRUE))\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-16-2.png){width=672}\n:::\n:::\n\n\nDie blaue Linie zeigt uns den *fit* (also die Regressionslinie) an, während die orangefarbene Linie ist ein *fit* durch die Residuen und sollte entlang der blauen Linie laufen und keine Kurvatur aufweisen.  \n\n\nEine weitere Alternative für die Abbildung der Originalwerte zusammen mit den Vorhersagewerten und Konfidenzintervalle des Modells bietet die `library(effects)`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nef=allEffects(mod, xlevels=100)\nef1=as.data.frame(ef[[1]])\nhead(ef1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Ert      fit        se    lower    upper\n1 51.65 16.31459 0.6778851 14.75139 17.87780\n2 52.23 16.25897 0.6687008 14.71694 17.80100\n3 52.81 16.20335 0.6595535 14.68241 17.72428\n4 53.39 16.14772 0.6504446 14.64779 17.64765\n5 53.97 16.09210 0.6413759 14.61308 17.57111\n6 54.56 16.03552 0.6321938 14.57767 17.49336\n```\n:::\n\n```{.r .cell-code}\ntail(ef1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Ert      fit        se    lower    upper\n95  106.4 11.06395 0.5005978 9.909565 12.21833\n96  107.0 11.00640 0.5089613 9.832737 12.18007\n97  107.5 10.95845 0.5159996 9.768556 12.14835\n98  108.1 10.90091 0.5245242 9.691357 12.11047\n99  108.7 10.84337 0.5331307 9.613969 12.07277\n100 109.3 10.78583 0.5418153 9.536401 12.03526\n```\n:::\n:::\n\n\nDurch das Argument `xlevels=100` werden 100 Einträge für die Erklärungsvariablen im beobachteten Werteberich erzeugt. Die Spalte `fit` zeigt die `fitted values` (Vorhersagewerte) und in `lower` und  `upper` sind die Grenzen des Konfidenzintervalls aufgeführt.  \n\nNun plotten wir die Originalwerte und zeichnen dann die Daten aus `ef1` mit den Funktionen `geom_line()` und `geom_ribbon()` ein.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reg, aes(x=Ert, y=Prot)) +\n  geom_point()+\n  geom_line(data = ef1, aes(x = Ert, y = fit))+\n  geom_ribbon(data = ef1, aes(x = Ert, y = NULL, ymin =lower, ymax = upper), alpha = 0.4)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-18-1.png){width=432}\n:::\n:::\n\n\n\n\nBei einer einfachen Regression kann man die Regressionslinie  mit der Funktion `geom_smooth(method=lm)` einzeichnen.  \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reg, aes(x=Ert, y=Prot)) +\n  geom_point()+\n  geom_smooth(method=lm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\noder `geom_abline(intercept = 21.26794, slope = -0.09590221)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reg, aes(x=Ert, y=Prot)) +\n  geom_point()+\n  geom_abline(intercept= 21.26794, slope=-0.09590221)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# oder so\nggplot(reg, aes(x=Ert, y=Prot)) +\n  geom_point()+\n  geom_abline(intercept=coef(mod)[1], slope=coef(mod)[2])\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n:::\n\n\nAlternativ, aber etwas komplizierter, geht es auch so:  \nUm die Regressionslinie des Modells in einen Plot \"per Hand\" einzuzeichnen, erstellen wir einen Testdatensatz, der alle Erklärungsvariablen des Modells in einem realistischen Wertebereich (Minimum bis Maximum) enthält. In diesem Beispiel ist das sehr einfach, da  `Ert` die einzige Erklärungsvariable ist.  \n\nWir benennen den Testdatensatz `td` und nutzen die `predict`-Funktion mit dem Argument `newdata=td` um die Erwartungswerte `td$p` zu berechnen. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(reg$Ert)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  51.64602 109.28159\n```\n:::\n\n```{.r .cell-code}\n#Testdatensatz mit Erklärungsvariablen (Wertebereich und Variablenname) erstellen \ntd<-data.frame(Ert=seq(from = 50, to =110, by = 5))\n\ntd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Ert\n1   50\n2   55\n3   60\n4   65\n5   70\n6   75\n7   80\n8   85\n9   90\n10  95\n11 100\n12 105\n13 110\n```\n:::\n\n```{.r .cell-code}\n#Predict-Funktion für neu erstellten Datensatz nutzen\ntd$p<-predict(mod, newdata=td)\ntd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Ert        p\n1   50 16.47283\n2   55 15.99332\n3   60 15.51381\n4   65 15.03430\n5   70 14.55479\n6   75 14.07528\n7   80 13.59576\n8   85 13.11625\n9   90 12.63674\n10  95 12.15723\n11 100 11.67772\n12 105 11.19821\n13 110 10.71870\n```\n:::\n\n```{.r .cell-code}\ntd<-data.frame(td, predict(mod, newdata=td, interval = \"confidence\"))\ntd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Ert        p      fit       lwr      upr\n1   50 16.47283 16.47283 14.848937 18.09672\n2   55 15.99332 15.99332 14.551201 17.43544\n3   60 15.51381 15.51381 14.245615 16.78200\n4   65 15.03430 15.03430 13.928471 16.14012\n5   70 14.55479 14.55479 13.593891 15.51568\n6   75 14.07528 14.07528 13.232828 14.91772\n7   80 13.59576 13.59576 12.832845 14.35868\n8   85 13.11625 13.11625 12.381199 13.85131\n9   90 12.63674 12.63674 11.872220 13.40126\n10  95 12.15723 12.15723 11.311883 13.00258\n11 100 11.67772 11.67772 10.713011 12.64243\n12 105 11.19821 11.19821 10.087965 12.30845\n13 110 10.71870 10.71870  9.445689 11.99171\n```\n:::\n\n```{.r .cell-code}\nstr(td)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t13 obs. of  5 variables:\n $ Ert: num  50 55 60 65 70 75 80 85 90 95 ...\n $ p  : num  16.5 16 15.5 15 14.6 ...\n $ fit: num  16.5 16 15.5 15 14.6 ...\n $ lwr: num  14.8 14.6 14.2 13.9 13.6 ...\n $ upr: num  18.1 17.4 16.8 16.1 15.5 ...\n```\n:::\n:::\n\n\nJetzt plotten wir die Originaldaten und zeichnen   \n\n+ die Regressionslinie durch die `geom_line()`-Funktion der vorhergesagten Werte `td$fit` und\n+ das Konfidenzintervall durch die `geom_ribbon`-Funktion der vorhergesagten Werte `td$lwr` und `td$upr` ein.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reg, aes(x=Ert, y=Prot)) +\n  geom_point()+\n  geom_line(data = td, aes(x = Ert, y = fit))+\n  geom_ribbon(data = td, aes(x = Ert, y = NULL, ymin = lwr, ymax = upr), alpha = 0.4)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n## Polynomiale Regression: Quadratischer Term  \n\nMit einer linearen Regression können auch \"nicht-lineare\" Zusammenhänge modelliert werden.  \n\n::: {.cell}\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-23-1.png){width=576}\n:::\n:::\n\n\nWenn die Daten einen nicht-linearen Trend aufweisen, wir aber nur einen linearen Term modellieren, zeigen die Residuen ein Muster, i.e. eine Kurvatur.    \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ndf=data.frame(y3=y3+runif(length(y3),0,4), x)\nmod=lm(y3~x, df)\nggplot(data=df, aes(y=y3, x=x))+\n  geom_point()+geom_smooth(method = lm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsimulationOutput <- simulateResiduals(fittedModel = mod, plot = F)\nplot(simulationOutput)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-24-2.png){width=672}\n:::\n:::\n\n\n\nWir können diesen Zusammenhang modellieren, indem wir einen quadratischen Term in das lineare Modell nehmen.  \n\n`mod<-lm(Abhängige~poly(Erklärungsvariable, 2), data=md)`  \noder  \n`mod<-lm(Abhängige~Erklärungsvariable+I(Erklärungsvariable^2), data=md)`  \n\nDas I (Großbuchstabe i) steht für \"as is\". \n\n::: {.cell}\n\n```{.r .cell-code}\nmod2=lm(y3~x+I(x^2), df)\nggplot(data=df, aes(y=y3, x=x))+\n  geom_point()+\n  geom_smooth(method = lm, formula=y ~ poly(x, 2))\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsimulationOutput <- simulateResiduals(fittedModel = mod2, plot = F)\nplot(simulationOutput)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplotResiduals(simulationOutput, form = df$x)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-25-3.png){width=672}\n:::\n:::\n\n\n\n## Übung 5\n\n+ Importiere die Daten [NDuenger.xlsx](https://github.com/DoreenGabriel/Kurs/blob/main/Themen/05/NDuenger.xlsx){target=\"_blank\"} und mach dich mit den Daten vertraut. \n\n::: {.callout-tip collapse=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(openxlsx)\ndat=read.xlsx(\"NDuenger.xlsx\")\nstr(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t20 obs. of  2 variables:\n $ ERT: num  70 73.1 75.5 79.8 81.1 79.7 84.5 82.9 83.6 86.4 ...\n $ ND : num  100 103 107 108 108 ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(dat, aes(x=ND, y=ERT)) +\t\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n  \n:::\n\n\n+ Führe eine Regression durch, um den Einfluss der N-Düngung auf den Ertrag zu modellieren. Was ist die Abhängige (y), was die Erklärungsvariable (x)?\n\n::: {.callout-tip collapse=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod<-lm(ERT~ND, data=dat)\ndrop1(mod, test=\"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nERT ~ ND\n       Df Sum of Sq    RSS    AIC F value    Pr(>F)    \n<none>               66.94 28.162                      \nND      1    911.81 978.75 79.811  245.17 6.271e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ERT ~ ND, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6108 -1.2027 -0.0947  1.6025  2.8800 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.66007    4.64358   2.726   0.0139 *  \nND           0.60648    0.03873  15.658 6.27e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.928 on 18 degrees of freedom\nMultiple R-squared:  0.9316,\tAdjusted R-squared:  0.9278 \nF-statistic: 245.2 on 1 and 18 DF,  p-value: 6.271e-12\n```\n:::\n:::\n\n:::\n\n+ Stimmen die Annahmen für eine lineare Regression?\n\n::: {.callout-tip collapse=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DHARMa)\nsimulationOutput <- simulateResiduals(fittedModel = mod, plot = F)\nplot(simulationOutput)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L,\n: Anpassung beendet mit Schrittweitenfehler - Ergebnisse sorgfältig prüfen\n```\n:::\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplotResiduals(simulationOutput, form=dat$ND)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L,\n: Anpassung beendet mit Schrittweitenfehler - Ergebnisse sorgfältig prüfen\n```\n:::\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-29-2.png){width=672}\n:::\n:::\n\n\nNein, eine deutliche Kurve in dem Residuen vs. fitted values Plot.\n\nIch fitte nun ein polynomiales Modell mit quadratischem Term. Man könnte auch die Abhängige und/oder die Erklärungsvariablen transformieren (z.B. Wurzel oder log) und dann Modelle fitten und die Residuen überprüfen. Ich denke aber, dass ein polynomiales Modell den Zusammenhang ganz gut widerspigeln könnte. \n \n\n::: {.cell}\n\n```{.r .cell-code}\nmod2<-lm(ERT~ND+I(ND^2), data=dat)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulationOutput <- simulateResiduals(fittedModel = mod2, plot = F)\nplot(simulationOutput)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplotResiduals(simulationOutput, form=dat$ND)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-31-2.png){width=672}\n:::\n:::\n\n\nBesser. Die Residuen weisen kein auffälliges Muster auf. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(mod2, test=\"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nERT ~ ND + I(ND^2)\n        Df Sum of Sq    RSS    AIC F value    Pr(>F)    \n<none>               42.324 20.992                      \nND       1    41.552 83.876 32.672 16.6897 0.0007708 ***\nI(ND^2)  1    24.619 66.943 28.162  9.8884 0.0059113 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nBeide Terme sind signifikannt. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ERT ~ ND + I(ND^2), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4642 -1.2745  0.1751  1.0468  3.1174 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.078e+02  3.849e+01  -2.800 0.012297 *  \nND           2.623e+00  6.421e-01   4.085 0.000771 ***\nI(ND^2)     -8.368e-03  2.661e-03  -3.145 0.005911 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.578 on 17 degrees of freedom\nMultiple R-squared:  0.9568,\tAdjusted R-squared:  0.9517 \nF-statistic: 188.1 on 2 and 17 DF,  p-value: 2.543e-12\n```\n:::\n:::\n\nDas Modell hat ein R² von 95,7%. \n\nWir können auch beide Modelle mit der anova()-Funktion (F-test) vergleichen und sehne hier, dass `mod2` signifikant besser fitted. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(mod, mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: ERT ~ ND\nModel 2: ERT ~ ND + I(ND^2)\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     18 66.943                                \n2     17 42.324  1    24.619 9.8884 0.005911 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nAuch ein Vergleich der AIC-Werte zeigt, dass `mod2` einen niedrigeren AIC im Vergleich zu `mod` hat und entsprechend damit einen besseren fit aufweist. Mehr Informationen zum AIC und Modellvergleich findest du im Kapitel [Statistische Modellierung](https://doreengabriel.github.io/Kurs/Themen/06/06_StatMod.html#modellselektion-basierend-auf-informationskriterien){target=\"_blank\"}. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(mod, mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     df      AIC\nmod   3 86.91963\nmod2  4 79.74999\n```\n:::\n:::\n\n:::\n\n+ Wie hoch ist laut Modell der zu erwartende Ertrag bei 120 kg N?\n\n::: {.callout-tip collapse=\"true\"}\n\nUm diese Frage zu beantworten, sollten wir das polynomiale Modell nutzen. Wir können entweder die 120 kg N in die Modellgleichung einsetzten\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)            ND       I(ND^2) \n-1.077851e+02  2.623078e+00 -8.368154e-03 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mod2)[1]+120*coef(mod2)[2]+120^2*coef(mod2)[3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n   86.48283 \n```\n:::\n:::\n\n\noder die Funktion `predict()`nutzen: \n\n::: {.cell}\n\n```{.r .cell-code}\npredict(mod2, newdata=data.frame(ND=120))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n86.48283 \n```\n:::\n:::\n\n\nWir sollten nicht das lineare Modell interpretieren, da dies keinen guten fit aufweist (trotz hohem R²). \n\n::: {.cell}\n\n```{.r .cell-code}\npredict(mod, newdata=data.frame(ND=120))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n85.43708 \n```\n:::\n:::\n\n\n\n:::\n\n+ Plotte die Regressionslinie des Modells/der Modelle.  \n\n::: {.callout-tip collapse=\"true\"}\n\nschnelle Interpretation: \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(effects)\t\nplot(allEffects(mod2))\t\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(c(\"ND\"), mod2, partial.residuals=TRUE))\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-40-2.png){width=672}\n:::\n:::\n\n\ndas \"falsche Model\"\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(allEffects(mod))\t\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(c(\"ND\"), mod, partial.residuals=TRUE))\t\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-41-2.png){width=672}\n:::\n:::\n\nIm Plot sehen wir nocheinmal, dass das euinfache lineare Modell (`mod`) im niedrigen und hohen N-Düngerbereich den Ertrag überschätzt. \n\n\nZu Interpretation dieses einfachen Modell (ohne weitere Kovariablen) könnten wir auch geom_smooth() nutzen, um die Regressionsline einzuzeichnen. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dat, aes(x=ND, y=ERT)) +\t\n  geom_point()+\t\n  geom_smooth(method=lm, formula=y ~ poly(x, 2), col=\"green\") # quadr\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\nnur zum Vergleich beider Modelle: \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dat, aes(x=ND, y=ERT)) +\t\n  geom_point()+\t\n  geom_smooth(method=lm)+#linear\n  geom_smooth(method=lm, formula=y ~ poly(x, 2), col=\"green\") # quadr\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\nhier aber noch der Code, bei dem das Modell interpretiert wird, wenn auch andere Effekte im Modell wären:  \n \n\n::: {.cell}\n\n```{.r .cell-code}\nef=allEffects(mod2, xlevels=100)\t\nef1=as.data.frame(ef[[1]])\t\nhead(ef1)\t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     ND      fit        se    lower    upper\n1 100.5 71.31378 1.0078085 69.18749 73.44007\n2 100.9 71.68888 0.9699969 69.64236 73.73539\n3 101.3 72.06129 0.9333186 70.09216 74.03042\n4 101.7 72.43103 0.8977950 70.53685 74.32521\n5 102.2 72.88943 0.8550494 71.08544 74.69343\n6 102.6 73.25315 0.8222100 71.51843 74.98786\n```\n:::\n\n```{.r .cell-code}\ntail(ef1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       ND      fit        se    lower    upper\n95  139.3 95.22989 0.9450695 93.23596 97.22381\n96  139.7 95.34523 0.9798871 93.27785 97.41261\n97  140.2 95.48565 1.0250315 93.32302 97.64827\n98  140.6 95.59497 1.0624230 93.35345 97.83648\n99  141.0 95.70161 1.1009301 93.37885 98.02437\n100 141.4 95.80557 1.1405367 93.39925 98.21190\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dat, aes(x=ND, y=ERT)) +\t\n  geom_point()+\n  geom_line(data = ef1, aes(x = ND, y = fit))+\t\n  geom_ribbon(data = ef1, aes(x = ND, y = NULL, ymin =lower, ymax = upper), alpha = 0.4)\t\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\noder so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntd<-data.frame(ND=seq(100,142, 1))\ntd<-data.frame(td, predict(mod, newdata=td, interval=\"confidence\"))\ntd<-data.frame(td, predict(mod2, newdata=td, interval=\"confidence\"))\ntd[1:10,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    ND      fit      lwr      upr    fit.1    lwr.1    upr.1\n1  100 73.30758 71.48955 75.12561 70.84115 68.61184 73.07046\n2  101 73.91405 72.16610 75.66200 71.78223 69.75529 73.80917\n3  102 74.52053 72.84164 76.19942 72.70657 70.86698 74.54617\n4  103 75.12700 73.51603 76.73798 73.61418 71.94615 75.28221\n5  104 75.73348 74.18911 77.27785 74.50505 72.99194 76.01816\n6  105 76.33995 74.86072 77.81919 75.37918 74.00340 76.75496\n7  106 76.94643 75.53064 78.36222 76.23658 74.97958 77.49358\n8  107 77.55290 76.19865 78.90716 77.07724 75.91965 78.23483\n9  108 78.15938 76.86446 79.45429 77.90117 76.82318 78.97915\n10 109 78.76585 77.52777 80.00394 78.70835 77.69035 79.72636\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dat, aes(x=ND, y=ERT)) +\t\n  geom_point()+\n  geom_line(data = td, aes(x = ND, y = fit.1))+\t\n  geom_ribbon(data = td, aes(x = ND, y = NULL, ymin = lwr.1, \n                             ymax = upr.1), alpha = 0.4)\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dat, aes(x=ND, y=ERT)) +\t\n  geom_point()+\n  geom_line(data = td, aes(x = ND, y = fit.1), size=1)+\t\n  geom_ribbon(data = td, aes(x = ND, y = NULL, ymin = lwr.1, \n                             ymax = upr.1), alpha = 0.2)+\n  geom_line(data = td, aes(x = ND, y = fit), col=2, size=1)+\t\n  geom_ribbon(data = td, aes(x = ND, y = NULL, ymin = lwr, \n                             ymax = upr, linetype=NA), col=2, alpha = 0.2)+\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nExtraaufgabe:  \n\n+ Extrapoliere die Vorhersage des Modells für eine N-Düngung von 250 kg. \n\n::: {.callout-tip collapse=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntd1<-data.frame(ND=seq(100,250, 1))\ntd1$p<-predict(mod, newdata=td1)\ntd1$p2<-predict(mod2, newdata=td1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dat, aes(x=ND, y=ERT)) +\t\n  geom_point()+\n  geom_line(data = td1, aes(x = ND, y = p))+\t\n  geom_line(data = td1, aes(x = ND, y = p2))\n```\n\n::: {.cell-output-display}\n![](05_Regression_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n:::\n\n\nWir sollten das Modell immer nur für den beobachteten Wertebereich interpretieren und insbesondere bei polynomialen Modellen nicht extrapolieren. \n\n:::\n\nEnde Übung 5  \n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "05_Regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}